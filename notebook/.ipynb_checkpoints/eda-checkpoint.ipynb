{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","private_outputs":true,"provenance":[],"mount_file_id":"1ZujM_uppI7YWoFtlpzB_fpPVpWMMzzqg","authorship_tag":"ABX9TyM5GBWYQoaa2k6CUFflMUqb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"EZOT8cvTBfYM"},"source":["cd /content/drive/MyDrive/Colab Notebooks/kaggle_shopeer/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ltLGsKi0B2jL"},"source":["import pandas as pd\n","# train&test.csvのデータ数\n","train = pd.read_csv(\"train.csv\")\n","test = pd.read_csv(\"test.csv\")\n","\n","print(\"train.shape\", train.shape, \"test.shape\", test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xiV9Vzn9AdFL"},"source":["# trainデータ表示（先頭3行)\n","train.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FJAHVYXPGA8S"},"source":["# testデータ表示（先頭3行)\n","test.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SkZ6Ts8MBqTq"},"source":["# train_images内の画像データを表示\n","from PIL import Image\n","\n","filename = \"train_images/0000a68812bc7e98c42888dfb1c07da0.jpg\"\n","# filename = \"train_images/00039780dfc94d01db8676fe789ecd05.jpg\"\n","imgPIL = Image.open(filename)  # 画像読み込み\n","\n","imgPIL"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jAteO5q2Ak2s"},"source":["!pip install tensorflow_addons"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-sV1WSx7YPtq"},"source":["!pip install efficientnet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VjhCVbxH1KqZ"},"source":["import re\n","import os\n","import numpy as np\n","import random\n","import math\n","import tensorflow as tf\n","import efficientnet.tfkeras as efn\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras import backend as K\n","import tensorflow_addons as tfa\n","from scipy import spatial\n","from tqdm.notebook import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HyEZFlvN1PXC"},"source":["# AUTOTUNEはGPUの処理とCPUの処理の配分を動的に設定してくれるパラメータ\n","AUTO = tf.data.experimental.AUTOTUNE\n","\n","# 構成\n","EPOCHS = 5\n","BATCH_SIZE = 32\n","IMAGE_SIZE = [384, 384]\n","SEED = 42\n","LR = 0.001\n","VERBOSE = 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7quwl7ZJA-tI"},"source":["# f1 score関数\n","def f1_score(y_true, y_pred):\n","    y_true = y_true.apply(lambda x: set(x.split()))\n","    y_pred = y_pred.apply(lambda x: set(x.split()))\n","    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n","    len_y_pred = y_pred.apply(lambda x: len(x)).values\n","    len_y_true = y_true.apply(lambda x: len(x)).values\n","    f1 = 2 * intersection / (len_y_pred + len_y_true)\n","    return f1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVxjwf8oBRLt"},"source":["def preprocess():\n","    ''' データから下記の値を抽出する関数\n","    ・train: 学習データ\n","    ・test:  テストデータ\n","    ・label_mapper：　    ラベルグループ,index\n","    ・label_mapper_inv：　index, ラベルグループ\n","    ・N_CLASSES:  ラベルグループ数\n","    ・ground_truth: \n","\n","    parameter\n","    ----------------------------------------\n","    なし\n","\n","\n","\n","    train = pd.read_csv('train.csv')\n","    test = pd.read_csv('test.csv')\n","\n","    # 重複した行を削除\n","    train.drop_duplicates(subset = ['image'], inplace = True)\n","    train.reset_index(drop = True, inplace = True)\n","    label_mapper = dict(zip(train['label_group'].unique(), np.arange(len(train['label_group'].unique()))))\n","    label_mapper_inv = dict(zip(np.arange(len(train['label_group'].unique())), train['label_group'].unique()))\n","    train['label_group'] = train['label_group'].map(label_mapper)\n","\n","    # クラス数\n","    N_CLASSES = train['label_group'].nunique()\n","\n","    # グラウンドトゥルースラベル形式を取得する\n","    tmp = train.groupby(['label_group'])['posting_id'].unique().to_dict()\n","    train['matches'] = train['label_group'].map(tmp)\n","    train['matches'] = train['matches'].apply(lambda x: ' '.join(x))\n","    ground_truth = train[['posting_id', 'matches']]\n","\n","    # セルフポストを使用してナイーブスコアを計算する\n","    ground_truth['f1'] = f1_score(ground_truth['matches'], ground_truth['posting_id'])\n","    score = ground_truth['f1'].mean()\n","    print(f'Using the same posting id as prediction our f1 score is {score}')\n","\n","# return train, test, label_mapper, label_mapper_inv, N_CLASSES, ground_truth"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C2fZ_Fvsq7AE"},"source":["train[['posting_id', 'matches']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wVlX7MTmBWVI"},"source":["# Function to seed everything\n","def seed_everything(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    tf.random.set_seed(seed)\n","\n","# Function to decode our images\n","def decode_image(image_data):\n","    image = tf.image.decode_jpeg(image_data, channels = 3)\n","    image = tf.image.resize(image, IMAGE_SIZE)\n","    image = tf.cast(image, tf.float32) / 255.0\n","    return image\n","\n","# Function to read our image and return image, label_group\n","def read_image(image, label_group):\n","    image = tf.io.read_file(image)\n","    image = decode_image(image)\n","    return image, label_group\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nM_LGLNuBaFl"},"source":["# Function to get our training dataset\n","def get_training_dataset(image, label_group):\n","    dataset = tf.data.Dataset.from_tensor_slices((image, label_group))\n","    dataset = dataset.map(read_image, num_parallel_calls = AUTO)\n","    dataset = dataset.repeat()\n","    dataset = dataset.shuffle(2048)\n","    dataset = dataset.batch(BATCH_SIZE)\n","    dataset = dataset.prefetch(AUTO)\n","    return dataset\n","\n","# Function to get our validation dataset\n","def get_validation_dataset(image, label_group):\n","    dataset = tf.data.Dataset.from_tensor_slices((image, label_group))\n","    dataset = dataset.map(read_image, num_parallel_calls = AUTO)\n","    dataset = dataset.batch(BATCH_SIZE)\n","    dataset = dataset.prefetch(AUTO)\n","    return dataset\n","    \n","# Function to split our data into train and validation\n","def train_and_eval_split(image, label_group):\n","    trn_image, val_image, trn_labels, val_labels = train_test_split(image, label_group, random_state = SEED, shuffle = True)\n","    return trn_image, val_image, trn_labels, val_labels\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QVsvCNXLBev-"},"source":["# Function to create our EfficientNetB0 model\n","def get_model():\n","        \n","    inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3))\n","    x = efn.EfficientNetB0(include_top = False, weights = 'imagenet')(inp)\n","    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n","    x = tf.keras.layers.BatchNormalization()(x)\n","    x = tf.keras.layers.Dense(512, activation = 'relu')(x)\n","    output = tf.keras.layers.Dense(N_CLASSES, activation = 'softmax')(x)\n","\n","    model = tf.keras.models.Model(inputs = [inp], outputs = [output])\n","\n","    opt = tf.keras.optimizers.Adam(learning_rate = LR)\n","\n","    model.compile(\n","        optimizer = opt,\n","        loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n","        metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n","    )\n","\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lMsm_qGyBix9"},"source":["# Function for a custom learning rate scheduler with warmup and decay\n","def get_lr_callback():\n","    lr_start   = 0.000001\n","    lr_max     = 0.000005 * BATCH_SIZE\n","    lr_min     = 0.000001\n","    lr_ramp_ep = 5\n","    lr_sus_ep  = 0\n","    lr_decay   = 0.8\n","   \n","    def lrfn(epoch):\n","        if epoch < lr_ramp_ep:\n","            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n","        elif epoch < lr_ramp_ep + lr_sus_ep:\n","            lr = lr_max    \n","        else:\n","            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n","        return lr\n","\n","    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = False)\n","    return lr_callback\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1SBg2dk6Bns6"},"source":["# Function to train and evaluate our model\n","def train_and_evaluate(image, label_group):\n","    print('\\n')\n","    print('-'*50)\n","    # Seed everything\n","    seed_everything(SEED)\n","    STEPS_PER_EPOCH = len(image) // BATCH_SIZE\n","    K.clear_session()\n","    model = get_model()\n","    image = 'train_images/' + image\n","    trn_image, val_image, trn_labels, val_labels = train_and_eval_split(image, label_group)\n","    train_dataset = get_training_dataset(trn_image, trn_labels)\n","    val_dataset = get_validation_dataset(val_image, val_labels)\n","    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'EfficientNetB0_{IMAGE_SIZE[0]}_{SEED}.h5', \n","                                                    monitor = 'val_loss', \n","                                                    verbose = VERBOSE, \n","                                                    save_best_only = True,\n","                                                    save_weights_only = True, \n","                                                    mode = 'min')\n","    history = model.fit(train_dataset,\n","                        steps_per_epoch = STEPS_PER_EPOCH,\n","                        epochs = EPOCHS,\n","                        callbacks = [checkpoint, get_lr_callback()], \n","                        validation_data = val_dataset,\n","                        verbose = VERBOSE)\n","    \n","    \n","    print('\\n')\n","    print('-'*50)\n","    print('Training Complete...')\n","    \n","    return model, val_image\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OFvVGuGCBvqe"},"source":["def get_cv_score(image, label_group, model, val_image):\n","    \n","    model.load_weights(f'EfficientNetB0_{IMAGE_SIZE[0]}_{SEED}.h5')\n","    model = tf.keras.models.Model(inputs = model.input, outputs = model.layers[-2].output)\n","    \n","    # Respect order\n","    image = 'train_images/' + image\n","    dataset_images = get_validation_dataset(image, label_group)\n","    dataset_images = dataset_images.map(lambda image, label_group: image)\n","    # Predict the entire dataset\n","    embeddings = model.predict(dataset_images)\n","    \n","    # Find the best threshold (lazy optimization)\n","    predictions_08 = []\n","    predictions_09 = []\n","    predictions_10 = []\n","    predictions_11 = []\n","    predictions_12 = []\n","    predictions_13 = []\n","    predictions_14 = []\n","    predictions_15 = []\n","    predictions_16 = []\n","    # Iterate over each validation image and use cosine distance to find similar images\n","    for val_index in tqdm(val_image.index):\n","        distances = spatial.distance.cdist(\n","            embeddings[np.newaxis, val_index, :], embeddings, 'cosine')[0]\n","        # Only get small distances\n","        TOP_08 = len(distances[distances <= 0.08])\n","        TOP_09 = len(distances[distances <= 0.09])\n","        TOP_10 = len(distances[distances <= 0.10])\n","        TOP_11 = len(distances[distances <= 0.11])\n","        TOP_12 = len(distances[distances <= 0.12])\n","        TOP_13 = len(distances[distances <= 0.13])\n","        TOP_14 = len(distances[distances <= 0.14])\n","        TOP_15 = len(distances[distances <= 0.15])\n","        TOP_16 = len(distances[distances <= 0.16])\n","        top_k_08 = list(np.argsort(distances)[:TOP_08])\n","        top_k_09 = list(np.argsort(distances)[:TOP_09])\n","        top_k_10 = list(np.argsort(distances)[:TOP_10])\n","        top_k_11 = list(np.argsort(distances)[:TOP_11])\n","        top_k_12 = list(np.argsort(distances)[:TOP_12])\n","        top_k_13 = list(np.argsort(distances)[:TOP_13])\n","        top_k_14 = list(np.argsort(distances)[:TOP_14])\n","        top_k_15 = list(np.argsort(distances)[:TOP_15])\n","        top_k_16 = list(np.argsort(distances)[:TOP_16])\n","        predictions_08.append(' '.join(train['posting_id'].iloc[top_k_08].values))\n","        predictions_09.append(' '.join(train['posting_id'].iloc[top_k_09].values))\n","        predictions_10.append(' '.join(train['posting_id'].iloc[top_k_10].values))\n","        predictions_11.append(' '.join(train['posting_id'].iloc[top_k_11].values))\n","        predictions_12.append(' '.join(train['posting_id'].iloc[top_k_12].values))\n","        predictions_13.append(' '.join(train['posting_id'].iloc[top_k_13].values))\n","        predictions_14.append(' '.join(train['posting_id'].iloc[top_k_14].values))\n","        predictions_15.append(' '.join(train['posting_id'].iloc[top_k_15].values))\n","        predictions_16.append(' '.join(train['posting_id'].iloc[top_k_16].values))\n","\n","    val_predictions = ground_truth.loc[val_image.index]\n","    val_predictions['predictions_08'] = predictions_08\n","    val_predictions['predictions_09'] = predictions_09\n","    val_predictions['predictions_10'] = predictions_10\n","    val_predictions['predictions_11'] = predictions_11\n","    val_predictions['predictions_12'] = predictions_12\n","    val_predictions['predictions_13'] = predictions_13\n","    val_predictions['predictions_14'] = predictions_14\n","    val_predictions['predictions_15'] = predictions_15\n","    val_predictions['predictions_16'] = predictions_16\n","    val_predictions['f1_08'] = f1_score(val_predictions['matches'], val_predictions['predictions_08'])\n","    val_predictions['f1_09'] = f1_score(val_predictions['matches'], val_predictions['predictions_09'])\n","    val_predictions['f1_10'] = f1_score(val_predictions['matches'], val_predictions['predictions_10'])\n","    val_predictions['f1_11'] = f1_score(val_predictions['matches'], val_predictions['predictions_11'])\n","    val_predictions['f1_12'] = f1_score(val_predictions['matches'], val_predictions['predictions_12'])\n","    val_predictions['f1_13'] = f1_score(val_predictions['matches'], val_predictions['predictions_13'])\n","    val_predictions['f1_14'] = f1_score(val_predictions['matches'], val_predictions['predictions_14'])\n","    val_predictions['f1_15'] = f1_score(val_predictions['matches'], val_predictions['predictions_15'])\n","    val_predictions['f1_16'] = f1_score(val_predictions['matches'], val_predictions['predictions_16'])\n","    print('Our f1 score with threshold 0.08 for the validation set is {}'.format(val_predictions['f1_08'].mean()))\n","    print('Our f1 score with threshold 0.09 for the validation set is {}'.format(val_predictions['f1_09'].mean()))\n","    print('Our f1 score with threshold 0.10 for the validation set is {}'.format(val_predictions['f1_10'].mean()))\n","    print('Our f1 score with threshold 0.11 for the validation set is {}'.format(val_predictions['f1_11'].mean()))\n","    print('Our f1 score with threshold 0.12 for the validation set is {}'.format(val_predictions['f1_12'].mean()))\n","    print('Our f1 score with threshold 0.13 for the validation set is {}'.format(val_predictions['f1_13'].mean()))\n","    print('Our f1 score with threshold 0.14 for the validation set is {}'.format(val_predictions['f1_14'].mean()))\n","    print('Our f1 score with threshold 0.15 for the validation set is {}'.format(val_predictions['f1_15'].mean()))\n","    print('Our f1 score with threshold 0.16 for the validation set is {}'.format(val_predictions['f1_16'].mean()))\n","    return val_predictions\n","\n","train, test, label_mapper, label_mapper_inv, N_CLASSES, ground_truth = preprocess()\n","model, val_image = train_and_evaluate(train['image'], train['label_group'])a\n","val_predictions = get_cv_score(train['image'], train['label_group'], model, val_image)\n","val_predictions.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MWHK5aoLB6NW"},"source":[""],"execution_count":null,"outputs":[]}]}